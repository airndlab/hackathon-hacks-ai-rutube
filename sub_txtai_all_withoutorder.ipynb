{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd664508-36a3-4429-9476-fae0d865a533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "pl.Config(fmt_str_lengths=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfc0ba-310c-4c7c-8c5e-2cada2ffb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa78a4-7ed3-4a00-9c61-58963687530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pl.read_csv('/home/jovyan/smirnov/our_submissions/test_dataset_submission_queries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff4b94-eb10-412b-845d-1212ae543296",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e7aea-4e59-493b-9d39-6e5e86c40796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import txtai\n",
    "embeddings = txtai.Embeddings()\n",
    "embeddings.load(\"/home/jovyan/smirnov/our_indexes/all_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68091f42-ca8e-4c58-a6ea-f40f8a0fcf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.search('Экстрасенсы. Битва сильнейших, 4 выпуск',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80221a68-dece-4a73-a2bb-6aeb218401cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Экстрасенсы. Битва сильнейших, 4 выпуск 2023'\n",
    "limit=5\n",
    "embeddings.search(f'select id, text, v_pub_datetime, score from txtai where similar (\"{q}\") \\\n",
    "order by score desc, v_pub_datetime desc \\\n",
    "limit {limit}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54985a4b-6c56-432d-8e22-b1c1a5424c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a53bc6-558f-45e7-a711-ccf8639ad410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "submission={\"query\" :[], \"video_id\":[]}\n",
    "for q in test['query']:\n",
    "    #print(q)\n",
    "    q_clean=re.sub(\"[^а-яА-ЯЁё0-9a-zA-Z ]\",\"\",q)\n",
    "    #print(q)\n",
    "    limit=5\n",
    "    res = embeddings.search(f'select id, text, v_pub_datetime, score from txtai where similar (\"{q_clean}\") \\\n",
    "    order by score desc \\\n",
    "    limit {limit}')\n",
    "    submission['query'] +=[q]*len(res)\n",
    "    submission['video_id'] +=(list(map(lambda x: x['id'],res)))\n",
    "\n",
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39f861c1-987b-410d-8a4f-341b5c9af60b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:19:13.661247Z",
     "iopub.status.busy": "2023-11-25T09:19:13.661021Z",
     "iopub.status.idle": "2023-11-25T09:19:15.312614Z",
     "shell.execute_reply": "2023-11-25T09:19:15.311809Z",
     "shell.execute_reply.started": "2023-11-25T09:19:13.661230Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/jovyan/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular')\n",
    "# Очистка текста (пример функции)\n",
    "def clean_text(text):\n",
    "    # Удаление специальных символов и приведение к нижнему регистру\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Удаление стоп-слов\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('russian')]\n",
    "    # Лемматизация\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460af41-029f-46e1-a688-148279894a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040064d-41d0-4043-9b09-6cffcf12f98b",
   "metadata": {},
   "source": [
    "## Удаление стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16abeed-23ec-43a7-bb32-0c76981ec1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T09:19:44.415018Z",
     "iopub.status.busy": "2023-11-25T09:19:44.414525Z",
     "iopub.status.idle": "2023-11-25T09:32:20.962650Z",
     "shell.execute_reply": "2023-11-25T09:32:20.961978Z",
     "shell.execute_reply.started": "2023-11-25T09:19:44.414996Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "submission = {\"query\": [], \"video_id\": []}\n",
    "limit = 5  # Fixed number of results per query\n",
    "for q in test['query']:\n",
    "    q_clean = clean_text(re.sub(\"[^а-яА-ЯЁё0-9a-zA-Z ]\", \"\", q))\n",
    "\n",
    "    # Execute search\n",
    "    res = embeddings.search(f'select id, text, v_pub_datetime, score from txtai where similar (\"{q_clean}\") \\\n",
    "                            order by score desc \\\n",
    "                            limit {limit}')\n",
    "\n",
    "    # If there are results, extend the submission dictionary with these results\n",
    "    submission['query'] += [q] * 5\n",
    "    submission['video_id'] += [x['id'] for x in res]\n",
    "\n",
    "    # If the results are fewer than the limit, fill the remaining slots with empty values\n",
    "    if len(res) < limit:\n",
    "        submission['video_id'] += [None] * (limit - len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b3d91c5-78ac-460b-bfd4-bde464c894cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T10:11:01.802978Z",
     "iopub.status.busy": "2023-11-25T10:11:01.802376Z",
     "iopub.status.idle": "2023-11-25T10:11:01.807458Z",
     "shell.execute_reply": "2023-11-25T10:11:01.806685Z",
     "shell.execute_reply.started": "2023-11-25T10:11:01.802955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ca60d-3d3b-4727-a773-732314c2bd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_colwidth', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43999ac7-e2a3-4dc0-a925-2568d882ac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1a029-896d-450f-abdc-b5876c573dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(submission)\n",
    "sub_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd6cc3c-27af-434b-a086-acc9d3a6f7aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-25T10:12:02.165430Z",
     "iopub.status.busy": "2023-11-25T10:12:02.164580Z",
     "iopub.status.idle": "2023-11-25T10:12:02.207583Z",
     "shell.execute_reply": "2023-11-25T10:12:02.206845Z",
     "shell.execute_reply.started": "2023-11-25T10:12:02.165374Z"
    }
   },
   "outputs": [],
   "source": [
    "sub_df.to_csv('/home/jovyan/smirnov/our_submissions/txtai_all_submission_03_without_order_with_stop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bb938d-0d56-472b-a32c-1f2f98135ef6",
   "metadata": {},
   "source": [
    "## Очистка стоп слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4c76f-ee99-4dd4-a41e-8d563ae31b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "submission={\"query\" :[], \"video_id\":[]}\n",
    "for q in test['query'].head(20):\n",
    "    #print(q)\n",
    "    q_clean=re.sub(\"[^а-яА-ЯЁё0-9a-zA-Z ]\",\"\",q)\n",
    "    print(q_clean)\n",
    "    limit=1\n",
    "    res = embeddings.search(f'select id, text, v_pub_datetime, score from txtai where similar (\"{q_clean}\") \\\n",
    "    order by score desc \\\n",
    "    limit {limit}')\n",
    "    submission['query'] +=[q]*len(res)\n",
    "    submission['video_id'] +=(list(map(lambda x: x['id'],res)))\n",
    "    #print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde89fb-877b-49fd-8fc8-a902cf1b9f94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
